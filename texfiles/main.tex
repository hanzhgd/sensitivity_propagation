%!TEX TS-program = xelatex

\documentclass[12pt]{article}

\usepackage{tgtermes}

% Table
\usepackage{booktabs}
\usepackage[flushleft]{threeparttable}

% Font
\usepackage[T1]{fontenc} % Font encoding
\usepackage{lmodern,microtype} % Typeface

% Math
\usepackage{amsmath,amsthm,amssymb,amsfonts} % AMS math
\usepackage{dsfont,mathrsfs,ushort} % Math style
\usepackage{mathtools}

\newtheorem{assp}{Assumption}

% Style
\usepackage{titlesec,titling} % Section titles
\usepackage[nohead]{geometry} % Page & margins
\usepackage{setspace} % Spacing
\usepackage{enumitem,booktabs} % Tables & lists

% References
%\usepackage[comma]{natbib} % Unused options: [sort] [sort&compress] [merge]
\usepackage[style=authoryear]{biblatex} 
\addbibresource{citation.bib}
\usepackage{thmtools,thm-restate}

% Addons
\usepackage{pstricks} % Figures
\usepackage{sgamevar} % Strategic form games

%example-continued
\usepackage{thmtools}
\declaretheorem[style=definition]{example}
\renewcommand\thmcontinues[1]{Continued}

% Institute
\usepackage[affil-it]{authblk}


% Hyperlinks
\usepackage{hyperref}

% suppress the black rectangle 
\overfullrule=0pt

\usepackage[splitrule]{footmisc} %% The splitrule option draws a full width rule above the continued part of the footnote as a visual cue to readers.
\interfootnotelinepenalty=10000 %% Completely prevent breaking of footnotes


\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{eg}{Example}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{rmk}{Remark}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\Indc}{\mathbb{I}}
\newcommand{\md}{\mathrm{d}}
\newcommand{\Ep}{\mathbb{E}}
\newcommand{ \littleop}{o_p}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Frontmatter
\title{\bfseries\Large Sensitivity Analysis for Estimation of Long-Term Treatment Effects \vspace*{-1ex}}
\author{\large\textit{Han Xu} \\
	\large \textup{Pennsylvania State University}\\\textup{hfx5023@psu.edu}}

\date{\today}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Style & Structure
% Hyperlinks
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
	pdftitle={sensitivity},
	pdfpagemode=FullScreen,
}

% Titles
\titleformat{\section}[block]{\centering\large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}[block]{\flushleft\bfseries}{\thesubsection.}{0.5em}{}
\titleformat{\subsubsection}[runin]{\normalsize\itshape}{\bfseries\thesubsubsection.}{0.5em}{}[.--\:]
\renewcommand{\thesubsubsection}{\arabic{section}.\arabic{subsection}.\alph{subsubsection}}
\titlespacing{\section}{0ex}{6ex}{3ex}
\titlespacing{\subsection}{0in}{3ex}{1.5ex}
\titlespacing{\subsubsection}{0mm}{2ex}{0.5em}
\renewcommand{\linespread}[1]{\setstretch{1}}
\linespread{2}
\begin{document}
	
	\maketitle
	
	\begin{abstract}
	\textcite{athey2020combining} proposed a data combination approach to identify the effect of treatment on long-term outcome, which relies on several key assumptions including unconfoundedness in an experimental sample, comparability of the experimental sample and an observational sample, and the latent unconfoundedness assumption in the observational sample. This paper shows how to construct bounds for the long-term treatment effects when the latent-unconfoundedness and sample comparability assumptions are weakened. For chosen sensitivity parameters, I derived an identified set (an interval characterizing the lower bound and upper bound for the ATE) for the average long-term treatment effect in the observational sample, which is often the parameter of interest in policy making.
    The bounds have simple and analytical characterization, and can be derived using a change-of-measure technique borrowed from \textcite{yadlowsky2018bounds}. 
    I proposed an MM(Method of Moment) estimator for the bounds, and an inference procedure that ensures valid coverage of the long-term treatment effects.
    Finally, I use a Monte Carlo experiment to illustrate the accuracy of estimation and inference procedure and its practical significance.
	\end{abstract}

    \section{INTRODUCTION}
    
    Estimation of the long-term effects of a policy or treatment is crucial since it lays the foundation for many real-world policy decisions, such as class size adjustment and poverty alleviation\footnote{see \textcite{dynarski2018closing,athey2020combining, dal2021information} for several applications where the long-term treatment effect is the parameter of interest.}. One challenge in estimation of the long-term effect is that it's often hard to use a randomized experiment to study the effect directly due to the delay of response. Recently, \textcite{athey2020combining} show that a combination of short-term experimental sample and long-term observational sample can help achieve identification (and also estimation) of long-term treatment effects. In the experimental sample, the researcher observes the randomized treatment and the short-term outcome($S$). In the observational sample, the researcher observes the confounded treatment, short-term outcome($S$), and the long-term outcome($Y$). The key assumptions for identifications are as follows: treatment in the experimental sample is unconfounded, treatment in the observational sample is latent-unconfounded, and the two samples are "comparable"--which is often referred to as external validity in the literature.
	
	However, in real-world applications, the above assumptions are subject to numerous problems. Although the unconfoundedness assumption in the experimental sample can be ensured if a randomization scheme is used, the other two assumptions may not hold for a variety of reasons. For example, the latent unconfoundedness assumption can fail when there are more independent confounding factors affecting treatment assignment than independent short-term outcome variables\footnote{See \textcite{athey2020combining} for a discussion.}. The sample comparability condition, on the other hand, can fail if the experimental sample is not representative of the target distribution, even after we control for covariates, which is usually referred to as selection bias in the literature. Moreover, it is widely known that without additional data or restrictions on the model, these assumptions are non-refutable. Namely, the data alone cannot tell us whether the assumptions are true. 
	
	Nonetheless, researchers interested in estimation of long-term treatment effects may want to assess how sensitive their results are to failures of these assumptions. Recent work by \textcite{masten2018identification, yadlowsky2018bounds, kallus2018confounding} each develops a nonparametric relaxation of the unconfoundedness assumption, which does not rely on auxiliary functional form or parametric distribution assumptions\footnote{Until recently, most approaches rely on strong auxiliary assumptions such as linearity of model or normality of the confounding factor, which can be undesirable because these auxiliary assumptions themselves is not needed for identification. }. Usually, the sensitivity analysis will produce a set of bounds corresponding to different choices of sensitivity parameters, and therefore allow researchers to evaluate how robust their results are to the weakening of assumptions. However, the current methods cannot be directly applied to the estimation of long-term treatment effects because of the presence of multiple interweaving assumptions: in deriving the identification result, the latent unconfoundedness assumption and the sample comparability assumption are used jointly, and we cannot derive valid bounds for the long-term treatment effects by analyzing the case where the assumptions are relaxed individually. The propagation of sensitivity is a new phenomena when we consider the relaxation of multiple assumptions. We might be interested in knowing how sensitive the results are to the relaxation of the latent unconfoundedness assumption when we've already relaxed the sample comparability assumption. Therefore, it is necessary to take into consideration the sensitivity parameter of the sample comparability assumption, while assessing sensitivity of the result to the latent unconfoundedness assumption. 
    %One possible extension to existing sensitivity analysis is to accommodate relaxation of multiple assumptions, which can be directly used to assess the sensitivity of results to the above collection of assumptions. 
    
	
	In this paper, I provide a method of sensitivity analysis for the when the estimation of long-term treatment effects using the method proposed by \textcite{athey2020combining}. I focus on the analysis of latent-unconfoundedness and sample comparability assumption, while keeping the unconfoundedness assumption maintained, since a randomization scheme is often used in the experimental sample.
    I apply the identification results of \textcite{yadlowsky2018bounds}, which is based on weakened likelihood-ratio restrictions on counterfactual outcome. The likelihood-ratio restriction is directly interpretable: it controls the dependency of odds-ratio of assignment on unmeasured confounding factors. For each assumption, I introduce a sensitivity parameter to measure how severely it is violated. Extending the single-sample case, I derive bounds for the parameter of interest, the average treatment effect (ATE) on the long-term outcome. Regarding the propagation of sensitivity, the current method allows the relaxation of one assumption to be complementary for the relaxation of another assumption in terms of bounding the target parameter.
	
	%Many interesting problems arise as one think about assessing sensitivity to multiple assumptions.  Another problem concerns the interpretation on sharpness of bounds. When there is only one assumption to be relaxed, many existing methods provide bounds that are sharp, i.e., that's attainable by some DGP that's consistent with the choice of sensitivity parameters. However, when there are multiple assumptions to be relaxed, the sharpness of bounds is more subtle.
	
	Based on the identification result, I also study the estimation and inference of bounds in the data combination context.
    I translate the defining equations of bounds into a set of moment conditions and propose a Method of Moment(MM) estimator. 
    When $S$ takes only discrete values, the asymptotic property of this MM estimator is derived. An application of delta method then gives the asymptotic distribution of bounds on $\tau$. The case where $S$ can take a continuum of values is more complicated, and is not covered in the current draft.
    With the estimation procedure, empirical researchers interested in using ACI's method to estimate long-term treatment effects can use these estimated identified sets (and accompanying confidence interval) to examine how sensitive their parameter estimates are to deviations from the baseline assumption of latent unconfoundedness and sample comparability. 
    Finally, I provide an example involving simulated data to illustrate the accuracy of estimation and inference procedure and its practical significance. 
	

	\medskip
	\textbf{Related Literature} In the rest of this section, I review the related literature. A directly related paper is the work by \textcite{athey2020combining}, which pioneers the use of latent unconfoundedness assumption. They show how one can combine experimental and observational samples to achieve identification of long-term treatment effects, when there exists short-term outcome that is helpful for controlling the confounding bias. A previous paper studies the same problem under the stronger assumption of surrogacy(see \textcite{athey2019surrogate}). \textcite{chen2021semiparametric} derive semiparametrically efficient estimator in these data combination setting. 

    There is a growing literature on sensitivity analysis for irrefutable assumptions, which stems from the analysis of causal inference in a potential outcome framework. A review for relaxation conditional independence by modeling the conditional probabilities of treatment assignment given both observable and unobservable variables is given in \textcite{masten2018identification}. \textcite{rosenbaum2002overt} proposed a sensitivity analysis within the context of randomization inference for testing the sharp null hypotheses of no unit level treatment effects for all units in one’s data set.The method proposed by this paper is closely related to \textcite{yadlowsky2018bounds}, which generalize the idea proposed by \textcite{rosenbaum2002overt}. Their idea is to weaken the assumption of unconfoundedness by relaxing the restriction on likelihood ratio for observable outcome and outcome under the counterfactual. The study of sample comparability\footnote{It is also often referred to as external validity in the literature.} is also closely related to the sensitivity analysis approach. There has been many studies on external validity and transportability of experimental results in social science (see \textcite{pearl2014external,bareinboim2016causal} for a treatment from the statistical community; recent economic researches includes  \textcite{allcott2015site,gechter2016generalizing,gechter2021combining} among many others). Interestingly, the same method could be used to weaken not only the confoundedness assumption, but also the latent-unconfoundedness and sample comparability assumption. 

    Meanwhile, relaxation of multiple assumptions introduces the problem of sensitivity propagation, which is not fully discussed in existing works. \textcite{masten2021salvaging} briefly discuss the implications of relaxing multiple assumptions in a instrumental-variable framework, and base their analysis on linear programming. However, their discussion are mainly based on a parametric IV model.

	\section{MODEL, ASSUMPTIONS, AND INTERPRETATION}
    To study the partial identification of long-term treatment effects under weakened assumptions, I set up the notation and some maintained assumptions. I follow \textcite{athey2020combining} closely in setup and notation(For convenience, I refer to the paper as ACI henceforth). I state the key assumptions which point identify the long-term treatment effects which are given in ACI. I then discuss how I relax those assumptions. Identified sets under these relaxations are derived in Section 3. I conclude this section by providing an interpretation of the deviations from baseline assumptions.
	
	
	\subsection{Notation}
	There are two available samples. Following \textcite{athey2020combining}, I will call them the experimental sample and the observational sample. I use $G$ to denote sample(group), $G=E$ for experimental and $G=O$ for observational. There is a single treatment $D$ that we are interested in, which takes binary values $0$ and $1$. The treatment effects model follows the “potential outcomes” approach of \textcite{rubin1974estimating}. There are two different sets of potential outcomes. The secondary (short-term) outcome, denoted by $S(d) \in \mathcal{S} \subset \mathbb{R}^k$ for $d\in \{0,1\}$, are auxiliary for identification.  The primary (long-term) outcome is denoted as $Y(d) \in R$. Here $Y = Y(1)D + Y(0)(1-D)$ and $S = S(1)D + S(0)(1 - D)$. In the experimental sample, we can observe $D$ and $S$, while in the observational sample we can observe $D, S$ and $Y$.
	For simplicity, I do not include any covariate, since the analysis for the with-covariate case is identical after conditioning on the covariates.

	\subsection{Review of Identifying Assumptions in ACI(2020)}
	The goal of \textcite{athey2020combining} is to identify $\Ep[Y(1) - Y(0) \mid G = O]$, the ATE in the observational sample. They make the following assumptions\footnote{In their work, they also include a covariate vector $X$, which we will omit here.}.
	\begin{assp}[Unconfoundedness in $G = E$]\label{a1}
		For $d \in\{0,1\}$,
		$$D_i \perp S_i(d) | G_i = E\footnote{Here I am working with "weak" unconfoundedness since I do not assume the joint independence of $S(0), S(1)$ and $D$. Unless mentioned, all unconfoundedness is weak unconfoundedness in this paper. For a discussion, see \textcite{imbens2000role}.}$$
	\end{assp}
		
	\begin{assp}[Sample comparability of $G = E$ and $G = O$]\label{a2}
		
		\begin{equation*}
		G_{i} \perp\left(Y_{i}(0), Y_{i}(1), S_{i}(0), S_{i}(1)\right)\footnote{In \textcite{athey2020combining}, they work with the following one called conditional sample comparability with $X_i$ being some additional covariates:
			$$
			G_{i} \perp\left(Y_{i}(0), Y_{i}(1), S_{i}(0), S_{i}(1)\right) \mid X_{i}
			$$It can be seen that it's just a conditional version of the current one. To simplify analysis, I will use the direct comparability throughout.}
		\end{equation*}
	\end{assp}
	
	\begin{assp}[Latent Unconfoundedness]\label{a3}
		For $d \in\{0,1\}$,
		$$
		D_{i} \perp Y_{i}(d) \mid S_i(d), G_{i}=\mathrm{O}
		$$
	\end{assp}
	% Intuitively, what it says is that the effects of the treatment on the short-term and long-term outcomes have the same set of unobserved confounding variables\footnote{ \textcite{athey2020combining} compare this assumption to a control function in a non-parametric instrumental variables setting. They show that, for exact identification, the unobserved confounder in the observational sample cannot have a dimension higher than that of the secondary outcome. It's understood as a rank restriction or dimension restriction.}.
	
	ACI show that we can identify $\tau = \Ep[Y(1) - Y(0) \mid G = O]$ given the above assumptions. To inspire the derivation of bounds when the assumptions are relaxed, I include a proof of the result below.
	\begin{restatable}{thm}{identification}{}\label{iden}
			Under A1-A3, $\mathbb{E}[Y(1) \mid G = O]$ and $\mathbb{E}[Y(0) \mid G = O]$ are identified.
	\end{restatable}
	
	\begin{proof}
		I focus on identification of $\Ep[Y(1) \mid G = O]$, since the counterpart for $\Ep[Y(0) \mid G = O]$ can be dealt with similarly. 
		
		By the law of iterated expectation, $\mathbb{E}[Y_i(1) \mid G = O] = \mathbb{E}[Y_i(1) \mid D_i = 0, G = O] P(D_i = 0 \mid G = O) + \mathbb{E}[Y_i(1) \mid D_i = 1, \mid G = O] P(D_i = 1, \mid G = O)$. The only non-directly-identified part is $E[Y_i(1) \mid D_i = 0, \mid G = O]$. It is equal to 
		\begin{align*}
		& \mathbb{E}[\mathbb{E}[Y_i(1) \mid S_i(1), D_i = 0, G = O] \mid D_i = 0, G = O] \\
		& = \int\mathbb{E}[Y_i(1) \mid S_i(1) = s, D_i = 0, G = O]  dP_{S(1) \mid D = 0, G = O}(s)\\
		& = \int \mathbb{E}[Y_i(1) \mid S_i(1) = s, D_i = 1, G = O] dP_{S(1) \mid D = 0, G = O}(s) \\
		& = \int \mathbb{E}[Y_i \mid S_i = s, D_i = 1, G_i = O] dP_{S(1) \mid D = 0, G = O}(s)
		\end{align*}
		We need the experimental data to identify $P_{S(1) \mid D= 0, G = O}(s)$. This is possible since
		$$
		P_{S(1) \mid G = O}(s) = P_{S(1) \mid D= 0, G = O}(s) P(D = 0 \mid G = O) +  P_{S(1) \mid D=1, G = O}(s) P(D = 1 \mid G = O)
		$$
		and $P_{S(1) \mid G = O}, P_{S(1) \mid D=1, G = O}(s)$ are identified from the experimental and observational data separately. Indeed, $P_{S(1) \mid G = O}(s) = P_{S(1) \mid G = E}(s) = P_{S(1) \mid D = 1, G = E}(s)$ by unconfoundedness and $P_{S(1) \mid D=1, G = O}(s) = P_{S \mid D = 1, G = O}(s)$, which is identified from the observational sample.
	\end{proof}
	
	In the proof above, the assumptions are used to perform changes of measure, or changes of the conditioning variables. More specifically, A\ref{a1} is used to show $P_{S(1) \mid G = E}(s) = P_{S(1) \mid D = 1, G= E}(s)$; A\ref{a2} is used to show that $P_{S(1) \mid G = E}(s) = P_{S(1) \mid G = O}(s)$; A\ref{a3} is used to show $ \mathbb{E}[Y_i(1) \mid S_i(1) = s, D_i = 1, G = O] = \mathbb{E}[Y_i(1) \mid S_i(1) = s, D_i = 0, G = O]$. Naturally, one might see that by relaxing these assumptions, the equality breaks and point identification of $\Ep[Y(1) \mid G = O]$ is no longer available.
	
	In \textcite{athey2020combining}, the author mention that it's often easy to ensure that A\ref{a1}(unconfoundedness in the experimental sample) is satisfied, since the researcher can often use randomization in assignment of treatment. However, the validity of A\ref{a2} and A\ref{a3} are not guaranteed. For example, the latent unconfoundedness assumption can fail when there are more independent confounding factors affecting treatment assignment than independent short-term outcome variables\footnote{See \textcite{athey2020combining} for a discussion.}. The sample comparability condition, on the other hand, can fail if the experimental sample is not representative of the target distribution, even after we control for covariates, which is usually referred to as selection bias in the literature. Also, it is widely known that without additional data or restrictions on the model, these assumptions are non-refutable. Namely, the data alone cannot tell us whether the assumptions are true. In this case, the researcher has several options: collect more data, make stronger assumptions or perform sensitivity analysis. I consider the last option in this paper. More specifically, I consider the relaxation of A\ref{a2} and A\ref{a3} below, since A\ref{a1}, as argued above, often holds when a randomization scheme is used. I now give the precise meaning relaxing A\ref{a2} and A\ref{a3}. 
	
	\subsection{Restrictions on Likelihood Ratio}
	Let $K$ and $H$ be real numbers greater than or equal to 1. The following definitions characterize relaxations of latent-unconfoundedness and sample comparability assumptions.
	
	\begin{defn}[$K-$(latent)-confoundedness] Let $L^d(y,s) = \frac{\mathrm{d} P(Y_i(d) = y \mid S_i(d) = s, D_i =1-d, G_i = O)}{\mathrm{d} P(Y_i(d) = y \mid S_i(d) = s, D_i = d, G_i = O)}$. Suppose $$L^d(y,s) \leq K L^d(y',s) \quad \forall y,y',s \text{ and } d\in\{0,1\}.$$ Then the distribution of primitives satisfy $K$-(latent)-confoundedness. In what follows, I will just refer to it as $K$-confoundedness. It relaxes the latent-unconfoundedness assumption in the observational sample.
	\end{defn}
	
	\begin{defn}[$H$-comparability]
	 Let $L^t(s) = \frac{\mathrm{d} P(S_i(t) = s \mid G_i = O)}{\mathrm{d} P(S_i(t) = s \mid G_i = E)}$. Suppose $$L^t(s) \leq H L^t(s') \quad \forall s,s' \text{ and } t \in \{0,1\}.$$ Then the distribution of primitives satisfy $H$-comparability. It relaxes the sample comparability assumption.
	\end{defn}

	When $K$ and $H$ are equal to 1, assumptions A\ref{a2} and A\ref{a3} holds. The restrictions are direct consequences of a change of measure, since, for example, $$\Ep[Y(1) \mid S(1) = s, D = 0, G = O] = \Ep[L^1(Y(1),s) Y(1) \mid S(1) = s, D = 1, G = O].$$ In fact, the likelihood ratio restriction is equivalent to a functional form restriction on the log odds ratio. 
	The following result adapted from \textcite{yadlowsky2018bounds} shows the case for $K$-confoundedness(and $d = 1$).
	
	\begin{restatable}{lem}{confound}
		\label{confound}
		Suppose $$Y(1) \perp D \mid S(1), U, G = O.$$
		The following are equivalent:
		\begin{enumerate}[label=(\alph*)]
			\item The log odds ratio has the following form $$\log \left(\frac{P(D=1 \mid S(1) = s, U=u, G = O)}{P(D=0 \mid S(1)=s, U=u, G = O)}\right)=\kappa(s)+\log (K) b(u)$$
			where $u$ stands for the unmeasured confounding variable, and $\norm{b}_{\infty} \leq 1$.

			\item For any fixed $s$, and any choice of $u$ and $u',$ $$\frac{P(D=1 \mid S(1) = s, U=u, G =O)}{P(D=0 \mid S(1)=s, U=u, G = O)} \times\frac{P(D=0 \mid S(1) = s, U=u', G =O)}{P(D=1 \mid S(1)=s, U=u', G = O)} \leq K.$$
			
			\item For any fixed $s$, let $L^1(y,s) = \frac{\mathrm{d} P(Y_i(1) = y \mid S_i(1) = s, D_i = 0, G = O)}{\mathrm{d} P(Y_i(1) = y \mid S_i(1) = s, D_i = 1, G = O)}$. Then $$L^1(y,s) \leq K L^1(y',s)$$ for any choice of $y$ and $y'$.
		\end{enumerate}
	\end{restatable}
	%\begin{proof}
	 %   The proof is essentially the same as those given in \textcite{rosenbaum2002overt}, if we treat $S(1)$ as a covariate variable.
	    
	%	Equivalence of $(a)$ and $(b)$ is given in \textcite{rosenbaum2002overt}. Equivalence of $(b) \text{ and } (c)$ is given in \textcite{yadlowsky2018bounds}.
	%\end{proof}
	Intuitively, $K-$confoundedness allows the assignment mechanism to depend on the unmeasured confounder to a certain degree characterized by sensitivity parameter $K$. When $K = 1$, the dependence is killed and the latent-unconfoundedness assumption is restored. Otherwise, the log-odd ratio of treatment assignment can be expressed as a function of a bounded transformation of the unmeasured confounder($U$), even after controlling for short-term outcome $S(1)$. Similarly, if we interpret $G_i$ as an assignment, and $S_i(d)$ as the target outcome, we can provide a set of equivalent conditions for $H-$comparability.

	When $K$ or $H$ is larger than $1$, and only $K-$confoundedness and $H-$comparability hold in the data, we can no longer point identify $\tau$. However, following recent works by \textcite{rosenbaum2002overt, masten2018identification,yadlowsky2018bounds}, it's possible to derive bounds for $\tau$ in this case, given the values of $K$ and $H$.
	
	\section{IDENTIFICATION}
	
	In this section, I study (partial) identification of long-term treatment effects when A\ref{a2} or A\ref{a3} are weakened. I start by introducing a lemma which is useful for bounding the mean of unavailable counterfactual outcome. I then apply these results to obtain bounds on $\Ep[Y(1) \mid G = O]$. The analysis for bounds on $\Ep[Y(0) \mid G = O]$ is similar and therefore omitted.
	
	\subsection{Bounds}
	
	The following lemma extends Lemma 2 in \textcite{yadlowsky2018bounds}, and is very useful for establishing the bounds. The original proof is a special case of the current one when $W(1)$ is scalar valued and $f$ is the identity map.
	
	\begin{restatable}{lem}{duality}
		\label{duality}
		Consider a potential outcome framework where the treatment variable is $D_i$ and the potential outcome is $W_i(1)$. Let $f:R^m \to R$ be a known $\sigma_{W(1)}$-measurable function and $$\mathbb{E}[\|f(W(1))\| \mid D=1] < \infty.$$
		Let $L(w) = \frac{\mathrm{d} P(W_i(1) = w \mid D_i = 0)}{\mathrm{d} P(W_i(1) = w \mid D_i = 1)}$.
		Consider the problem \begin{equation*}
		\begin{array}{ll}
		\inf _{L(w)} & \mathbb{E}[L(W(1))f(W(1)) \mid D=1] \\
		\text { s.t. } & \mathbb{E}[L(W(1)) \mid D=1]=1 \\
		& L(w) \geq 0, L(w) \leq B L(\tilde{w}) \text { for almost every } y, \tilde{y} \in \mathbb{R}.
		\end{array}
		\end{equation*}
		The solution to it is identical to 
		\begin{equation*}
		\begin{array}{ll}
		\sup _{\mu} & \inf _{L(w)} \quad \mathbb{E}[(f(W(1))-\mu) L(W(1)) \mid D=1]+\mu \\
		\text { s.t. } &\quad  L(w) \geq 0, L(w) \leq B L(\tilde{w}) \text { for almost every } w, \tilde{w} \in \mathbb{R}.
		\end{array}
		\end{equation*}
		which is further equal to
		\begin{equation*}
		\begin{array}{ll}
		\sup _{\mu} & \mu \\
		& \text { s.t. }  \mathbb{E}\left[(f(W(1))-\mu)_{+}-B(f(W(1))-\mu)_{-} \mid D=1\right] \geq 0.
		\end{array}
		\end{equation*}
		Moreover, the above inequality is actually an equality. The bound given here is sharp: there exists some $L$ such that the lower bound is attained.
	\end{restatable} 
	
	\begin{proof}
		See appendix. Note that the bound is sharp when $L(w)$
		takes only two values $\{\frac{B}{B + 1}, \frac{1}{B + 1}\}$ almost surely.
		
		Also, there is a counterpart for the upper bound:
		\begin{equation*}
		\begin{array}{ll}
		\sup _{L(w)} & \mathbb{E}[L(W(1))f(W(1)) \mid D=1] \\
		\text { s.t. } & \mathbb{E}[L(W(1)) \mid D=1]=1 \\
		& L(w) \geq 0, L(w) \leq B L(\tilde{w}) \text { for almost every } y, \tilde{y} \in \mathbb{R}.
		\end{array}
		\end{equation*}
		For this problem, the solution is identical to \begin{equation*}
		\begin{array}{ll}
		\inf _{\mu} & \mu \\
		& \text { s.t. }  \mathbb{E}\left[(\mu-f(W(1)))_{+}-B(\mu-f(W(1)))_{-} \mid D=1\right] \geq 0.
		\end{array}
		\end{equation*}
		
		To prove it, just replace $f$ by $-f$ and use the above lemma. Therefore the lemma also gives us an expression for the upper bound.
	\end{proof}
	
	Lemma \ref{duality} relates the restriction on the likelihood profile to bounds on mean of counterfactual outcome. When $B = 1$, the restriction is strong since it requires that the likelihood for observing certain $w$ to be equal for treatment status. In this case, $\mathbb{E}\left[(f(W(1))-\mu)_{+}-B(f(W(1))-\mu)_{-} \mid D=1\right] = \mathbb{E}\left[(f(W(1))-\mu) \mid D=1\right] = 0$, so we are back in the case where $\Ep[f(W(1))]$ can be identified. When $B$ is larger than 1, the restriction is relaxed, and the lower bound is smaller than $\Ep[f(W) \mid D = 1)]$. Also, it's readily seen as $B$ increase, the lower bound will get looser, so the bound on $\Ep[f(W(1)) \mid D = 0]$ will be monotonic in the sensitivity parameter $B$ here. 

	With these tools, we can establish bounds for $\Ep[Y(1)] \mid D = 0, G = O]$, which then allow us to bound $\Ep[Y(1)] \mid G = O]$. Take the lower bound of $\Ep[Y(1) \mid D= 1, G = O]$ as an example, we have the following theorem, whose proof is given in the appendix:
	\begin{restatable}[Lower Bound on $\Ep(Y(1) \mid D = 0, G = O)$]{thm}{lb}\label{thm:thm2}
		We maintain the assumption of randomization  in the experimental sample throughout.
		\begin{enumerate}[label=(\alph*)]
			\item Under $K$-confoundedness and sample comparability, we have the following lower bound for $\Ep[Y(1) \mid D = 0, G = O]$:
			$$\mu^-_1 = \int \theta_1(s) \mathrm{d} P_{S(1) | D = 0, G = O}(s)$$ where 
			\begin{equation}\label{eq:eq1}
			\begin{array}{ll}
			\theta_1(s) \coloneqq & \sup _{\mu}  \mu \\
			& \text { s.t. }  \mathbb{E}\left[(Y(1)-\mu)_{+}-K(Y(1)-\mu)_{-} \mid S(1)=s, D=1, G = O\right] \geq 0,
			\end{array}
			\end{equation}
			and 
			\begin{equation}\label{eq:eq2}
			P_{S(1) \mid D = 0, G = O} (\cdot) = \frac{P_{S \mid D = 1, G = E}(\cdot) - P_{S \mid D = 1, G = O} (\cdot) P(D = 1 \mid G = O) }{P(D = 0 \mid G = O)}.    
			\end{equation}
			
			
			\item Under $K-$latent-unconfoundedness and H-comparability, we have the following lower bound for $\Ep[Y(1) \mid D = 0]$:
			
			\begin{equation}\label{eq:eq3}
			\begin{array}{ll}
			\mu_1^- =  \frac{\zeta_1^- - \Ep[\theta_1(S) \mid D=1, G=O] P(D=1\mid G=O)}{P(D = 0 \mid G = O)}
			\end{array}
			\end{equation}
			where 
			\begin{equation}\label{eq:eq4}
			\begin{array}{ll}
			\zeta_1^- \coloneqq & \sup _{\mu}  \mu \\
			& \text { s.t. }  \mathbb{E}\left[(\theta_1(S)-\mu)_{+}-H(\theta_1(S))-\mu)_{-} \mid D=1, G = E\right] \geq 0,
			\end{array}
			\end{equation}
			
		\end{enumerate}
	\end{restatable}
	
	Theorem \ref{thm:thm2} shows the how the sensitivity is propagated through the relaxation of both assumption A\ref{a2} and assumption A\ref{a3}. In case (a), only the latent-unconfoundedness assumption is relaxed, and the lower bound for $\Ep[Y(1) \mid D = 0, G =O]$ is an conditional expectation of the lower bound $\theta_1(s)$; in case (b), the sensitivity margin introduced by relaxing the sample comparability assumption is also taken into account, which complements the lower bound $\theta_1(s)$. 
	
	A similar upper bound for $\Ep[Y(0) \mid D = 1, G= O]$ is available, by using the upper bound counterpart of lemma \ref{duality}. The result is given below.
	\begin{cor}
			Under $K$-confoundedness and $H-$comparability, we have the following identified upper bound for $\Ep[Y(0) \mid D = 1, G= O]$:
			$$
			\mu^+_0 =\frac{\zeta_0^+ - \Ep[\theta_0(S(0)) | D = 0, G = O] P(D = 0 \mid G = O)}{P(D = 1 \mid G = O)}
			$$
			where 
			\begin{equation}\label{eq:eq5}
			\begin{array}{ll}
			\theta_0(s) \coloneqq & \inf _{\mu}  \mu \\
			& \text { s.t. }  \mathbb{E}\left[(\mu - Y(0))_+ - K (\mu - Y(0))_- \mid S(0) = s, D = 0, G = O \right] \geq 0.
			\end{array}
			\end{equation}
			and
			\begin{equation}\label{eq:eq6}
			\begin{array}{ll}
			\zeta_0^+ \coloneqq & \inf _{\nu}  \nu \\
			& \text { s.t. }  \mathbb{E}\left[(-\theta_0(S)+\nu)_{+}-H(-\theta_1(S)+\nu)_{-} \mid D=0, G = E\right] \geq 0.
			\end{array}
			\end{equation}
	\end{cor}
	
	Using these bounds we obtain a lower bound for $\tau = \Ep[Y(1) - Y(0) \mid G = O]$:
	\begin{eqnarray*}
		\tau^-  & = & \left[\Ep[Y(1) \mid D = 1]P(D = 1) + \mu_1^-  P(D = 0)\right] - \\
		 & & \quad \left[\Ep[Y(0) \mid D = 0]P(D = 0) + \mu_0^+ P(D = 1)\right].
	\end{eqnarray*}
	
	It's also direct to extend these argument to get an upper bound for $\tau$. To avoid repetitiveness the argument is omitted here.
	
	\subsection{Sharpness of these bounds}
	It's important to remark on the sharpness of these bound. Consider the lower bound for $\Ep[Y(1) \mid D = 0, G = O]$. Under $K-$confoundedness and $H-$comparability, there exist unobservable confounder $U,V$ such that $$\log \left(\frac{P(D=1 \mid S(1) = s, U=u, G = O)}{P(D=0 \mid S(1)=s, U=u, G = O)}\right)=\kappa(s)+\log (K) b(u)$$ with $\norm{b}_{\infty} \leq 1$, and 
	$$\log \left(\frac{P(G=E \mid V=v)}{P(G=O \mid V=v)}\right)=\kappa'+\log (H) b'(v)$$ with $\norm{b'}_{\infty} \leq 1$. Note that $\theta_1(s)$ is attained when
	$$L(y,s) = \frac{\mathrm{d} P(Y_i(1) = y \mid S_i(1) = s, D_i =0, G_i = O)}{\mathrm{d} P(Y_i(1) = y \mid S_i(1) = s, D_i = 1, G_i = O)}$$ takes only two values  $\{\frac{K}{K + 1}, \frac{1}{K + 1}\}$ almost surely, and $\zeta_1^-$ is attained when  
	$$L(s) = \frac{\mathrm{d} P(S_i(1) = s \mid G_i = O)}{\mathrm{d} P(S_i(1) = s \mid , G_i = E)}$$
	takes only two values  $\{\frac{H}{H + 1}, \frac{1}{H + 1}\}$ almost surely. It's possible these conditions hold marginally but not jointly\footnote{An extreme case is that $U = V$.}. When $U$ and $V$ are independent, these bounds are sharp.
	
	Therefore, if a DGP satisfies both $K-$confoundedness and $H-$comparability is sharp for each of them, our bounds can still be too not sharp because of potential correlation (more generally, dependence) of confounders. Compared to existing sharp bounds on the treatment effect, this is an undesirable feature. The case where the confounders are correlated is left as a future research question.
	
% 	\begin{rmk}[Monotonicity wrt to $K$ or $H$]
% 		It is readily seen that as $K$ or $H$ gets larger, the bound gets looser. Indeed, suppose $K$ gets larger, then in equantion \ref{eq:eq1}:
% 		\begin{equation*}
% 		\begin{array}{ll}
% 			\theta_1(s) \coloneqq & \sup _{\mu}  \mu \\
% 			& \text { s.t. }  \mathbb{E}\left[(f(Y(1))-\mu)_{+}-K(f(Y(1))-\mu)_{-} \mid S(1)=s, D=1\right] \geq 0,
% 		\end{array}
% 		\end{equation*}
% 		an increase in $K$ decreases the value of the left hand side, so it must be compensated by a decrease in $\mu$. Analysis for other cases are similar. Therefore, it is confirmed that increased $K$(or $H$) leads to looser bounds.
% 	\end{rmk}

% 	\begin{rmk}[Weighted Average]
% 		I derive an alternative expression for $\zeta_1$ here. To fix ideas, consider the case where $s\in\{s_1, s_2\}$. In this case, $\zeta_1$ satisfy the following equation:
% 		\begin{align*}
% 		& P(S(1)=s_1 \mid T = 1)\left[(\theta_1(s_1)-\nu)_{+}-H(\theta_1(s_1)-\nu)_{-} \right] + \\ & \quad P(S(1)=s_2 \mid T = 1) \left[(\theta_1(s_2)-\nu)_{+}-H(\theta_1(s_2)-\nu)_{-} \right] = 0
% 		\end{align*}
% 		It is readily seen that $\zeta_1$ is the weight average of the solution to two separate equations. Simple algebra verifies that if $\theta_1(s_1) < \theta_1(s_2)$, then $$\zeta_1 = \frac{w_1Ha_1 - w_2a_2}{w_1H + w_2},$$ with $w_i = P(S(1) = s_i \mid T = 1)$ and $a_i = \theta_1(s_i)$, $i = 1,2$. 
% 		Here the weights depend on the distribution of $S(1)$, and a more probable $s$ contributes more to the determination of $\zeta$. This above form naturally extends to the case where we have continuous $S(1)$: we can just replace the probability mass function by probability density function, and the resulting $\zeta_1$ is a weighted average of $\{\theta_1(s)\}_s$. For each $s$ such that $\theta_1(s) < \zeta_1$, they are multiplied by an additional $H$ factor:
% 		$$\zeta_1 = \frac{\int w(s) H \theta_1(s) \Indc(\theta_1(s) < \zeta_1) - w(s) \theta_1(s)\Indc(\theta_1(s) \geq \zeta_1) \mathrm{d}s}{\int w(s) H \Indc(\theta_1(s) < \zeta_1) + w(s) \Indc(\theta_1(s) \geq \zeta_1) \mathrm{d}s},$$
% 		with $w_i = p_{S(1) \mid T}(s \mid T = 1)$. 
		
% 		This procedure shall also work for the analysis of $\theta_1(s)$, which will be omitted here. The weighted average expression also shows up in the formula of asymptotic variance.
% 	\end{rmk}

    \section{ESTIMATION AND INFERENCE}
	In this section, I investigate the estimation and inference of bounds for $\tau$ when $K$ and $H$ are given. To begin with, I consider the case where the range of $S$ is discrete. Extension to the general case requires a more sophisticated argument, and is not covered in the current draft.
	\subsection{Motivation}
	
	Equations \ref{eq:eq1} through \ref{eq:eq6} defines the key objects in the identification of bounds. It is also clear that they constitute a set of moment conditions, which suggests that we can just use the method of moment to estimate these parameters.
	\footnote{Consistency and asymptotic normality for the single-sample no-covariate case is given in existing literature. For example, see \textcite{yadlowsky2018bounds}. They use the same reasoning, but only for cases without covariates and multiple potential outcomes. For Z-estimator theory, see \textcite{van2000asymptotic} Chapter 5.}. In what follows, let $$\psi_x(y) = (y - x)_+ - K (y - x)_-$$ and $$\phi_x(y) = (y - x)_+ - H (y - x)_-.$$
	
	Suppose $S$ takes only finitely many values\footnote{This assumption ensures that we only need to deal with finitely many moment equations. The case where $S$ can }. Take the analysis of $S(1)$ and $Y(1)$ as an example. Suppose $S(1) \in \{s_1,\ldots, s_m\}$. For each $s_k, k\in\{1,\ldots,m\}$, estimation of $\theta_1(s_k)$ is straightforward:
	$$
	\mathbb{E}\left[\psi_{\hat{\theta}_1(s_k)}(Y) \mid S = s_k, D=1, G = O\right] = 0
	$$
	and thus by the principle of analogy, we might replace all $\Ep$ by its sample analogue $\Ep_n$ for $s_k$ to get an estimator for $\hat{\theta}_1(s_k)$. Namely, 
	$$
    \mathbb{E}_n\left[\psi_{\hat{\theta}_1(s_k)}(Y) \mid S = s_k, D=1, G=O\right] = 0
	$$
	Similarly, the equation 
	\begin{equation*}
	\mathbb{E}\left[\phi_{{\zeta}_1}(\theta_1(S)) \mid D=1,G=E\right] = 0
	\end{equation*} 
	has a natural sample counterpart:
	\begin{equation*}
	\mathbb{E}_n\left[\phi_{\hat{\zeta}_1}(\hat{\theta}_1(S)) \mid D=1, G=E\right] = 0.
	\end{equation*}
	Now, with estimators of $\theta_1$ and $\zeta_1$ at hand, one can easily construct estimators for $\tau$. One caveat here is that if $S$ is continuously distributed, the above procedure will fail since the event $\{S = s\}$ has probability 0. One could extend the current argument to deal with continuous $S$, but that is beyond the scope of this paper.
	
	\subsection{Method of Moment Estimator}
	To get a consistent and asymptotically normal estimator for $\tau^-(\tau^+)$ I make the following assumptions.
	
	\begin{assp}

		\begin{enumerate} In deriving the estimator, I make the following assumptions.
			\item $\mathcal{S}$, the support of $S$, is a finite discrete set.
			\item $F_{Y(d) \mid D = d, G = i}$ is absolutely continous for $i = G, O.$
		\end{enumerate}
	\end{assp}

	The first assumption, as argued above, is made because we want to use sample analogue of the equations \ref{eq:eq1} to equation \ref{eq:eq6}. The second one is assumed to avoid the problem caused by the non-differentiablity of $\psi_x$ and $\phi_x$. Note that although $\psi_x$ and $\phi_x$ themselves are non-differentiable(actually, only at one point $x$), after taking conditional expectation, the function is smooth enough. Too see this, consider the following simple example: 
		\begin{align*}
		\frac{\partial}{\partial \theta}\Ep[\psi_{\theta}(Y(1))] & = \frac{\partial}{\partial \theta} \Ep[(Y(1) - \theta)_+ - K (Y(1) - \theta)_-].
		\end{align*}
		We have
		$$\frac{\partial}{\partial \theta} \Ep[(Y(1) - \theta)_+] = F_{Y(1)}(\theta) - 1$$
		and 
		$$\frac{\partial}{\partial \theta} \Ep[K(Y(1) - \theta)_-] = K F_{Y(1)}(\theta) $$
		by the absolute continuity of $F_{Y(1)}$. Therefore, we can bypass the problem of non-differentiability in moment functions, as shown in \textcite{andrews1994asymptotics}, and still have consistency and asymptotic normality for the MM estimator. 
        In particular, one can get consistent and asymptotically normal estimators for $\theta$ and $\zeta$, which are important for the derivation of the bounds. \footnote{In the appendix, I show a more detailed argument that one can use to prove consistency and asymptotic normality in this case.}.
	
	The asymptotic distribution of $\hat{\tau}^-$ is given by the following expansion:
	\begin{equation}\label{eqn:deltamethod}
	    \begin{aligned}
            \hat{\tau}^{-} - \tau^- = \tilde{p}_1 (m_1 + m_0 - v_1^- - v_0^+) + (1- p_1)(\tilde{v}_1^- - \tilde{m}_0) + \\p_1 (\tilde{m}_1 - \tilde{v}_0^+) + o_p(\frac{1}{\sqrt{n}})
        \end{aligned}
	\end{equation}
    where $p_1 = P(D =1 \mid G = O)$, $v_1^- = \text{lower bound of } \mathbb{E}[Y(1) \mid D = 0, G = O]$, $v_0^+ = \text{upper bound of }\mathbb{E}[Y(0) \mid D = 1, G = O]$, $m_1 = \mathbb{E}[Y(1) \mid D = 1, G = O]$, similarly for $m_0$, and all the tilde terms are just estimator minus true value. 
    
    Take the estimation of $\hat{\tau}^-$ as an example, the moment conditions are: 
	\begin{itemize}
	    \item 	$p_1 = P(D = 1 \mid G = O)$
	    
	    \item $\mathbb{E}\left[\mathbb{I}(S=s, D=1, G = O) \psi_{\theta_1(s)}(Y)\right]  = 0$
	    
        \item $m_1 = \mathbb{E}[Y \mid D = 1, G = O]$
        
        \item $\mathbb{E}\left[\Indc(D=1,G=E)\phi_{{\zeta}_1}(\theta_1(S))\right] = 0$
        
        \item $q_s = P(S=s \mid D=1, G=O)$
	\end{itemize}
	Stacking them into one vector $g_n(\gamma), $ with $\gamma = (p_1, \theta_1, m_1, \zeta_1, q)$. Let $G = \frac{\partial g(\gamma)}{\partial \gamma}$. The asymptotic variance of $\hat{\gamma}$ is then given by $G'V^{-1}G$. Note that $\mu_1^- = \frac{\zeta_1^- - (\sum_s \theta_1(s) q_s)  \times p_1}{1 - p_1}$ and $v_1^- = p_1 m_1 + (1 - p_1) \mu_1^- $, so the asymptotic distribution of $\hat{v}_1^-$ is available by an application of the delta method. Repeat the process for $v_1^+$, we can then use the expansion in equation \ref{eqn:deltamethod} to get the asymptotic variance for $\hat{\tau}^-.$
	
	\subsection{A note on inference}
	The previous procedure can be used to construct confidence intervals for $\tau^-$ and $\tau+$. 
    Moreover, it can be shown that the two CI are independent. 
    However, policy maker/researchers are mostly interested in inference on the true value $\tau$, rather than the bounds of $\tau$. 
    There has been extensive work on how to conduct inference under partial identification(for example, see \textcite{Andrews2007InferenceFP, stoye2020simple}). 
    Here, I'll take a conservative approach: Let $[a_l, b_l]$ be the CI for $\tau^-$ with confidence level $1 - \alpha$, and $[a_h, b_h]$ be the CI for $\tau^+$ with confidence level $1 - \alpha$. 
    Then $[a_l, b_h]$ should cover the true parameter $\tau$ with high probability. 
    In particular, if $\tau^- = \tau^+$, i.e., assumptions A\ref{a1}-A\ref{a3} are all satisfied, $[a_l, b_l] = [a_h, b_h]$ and therefore the coverage rate of $[a_l, b_h]$ should be $1 - \alpha$. 
    When A\ref{a2} or A\ref{a3} fails, the proposed CI would be conservative, i.e., the coverage rate is larger than $1 - \alpha$.
	
	
	\section{SIMULATION}
	
	While the theoretical properties of the MM procedure ensure that the estimator is asymptotically normal and has valid asymptotic confidence intervals, I examine their finite sample performance below based on Monte-Carlo simulation. The details of numerical study are given in the next two subsections. But in summary, the Monte-Carlo simulation study supports the validity of the inference procedure in the specified setting. When $K = H = 1$, i.e., assumptions A\ref{a2} and A\ref{a3} are satisfied, the procedure gives pretty accurate coverage in finite sample. When both sensitivity parameters are larger than $1$, the procedure still gives a valid CI, but is overly conservative as shown in the simulation result.

	\subsection{DGP for the Simulation}
	Fix $K,H \geq 1$. Let $U$ be the confounder for sample comparability, and $U \sim N(0,1)$. Assume that $$S(0) \sim Bernoulli(0.5 + 0.15 \times \mathbb{I}(U > 0))$$ and $$S(1) \sim Bernoulli(0.5 + 0.3 \times \mathbb{I}(U > 0)).$$
    For any individual, let the probability of being in the observational sample be $$\mathbb{P}(G = E) = \frac{1}{1 + \exp (-\log(H) \times \mathbb{I}(U > 0) )},$$ so we have $H-$comparability. Also, on average, $S(i)$ is larger in the experimental sample than in the observational sample, 
    which may contribute to over-estimation of $\tau$ (since the ACI use $S(i)$ in the experimental sample to approximate the true distribution of $S(i)$ in the observational sample). 

    Let $V$ be the confounder for latent-confoundedness, and $V \sim N(0,1)$. Let $$Y(0) = S(0) + V + \varepsilon,$$ and $Y(1) = Y(0) + 1$, so the actual treatment effect on long-term outcome is constant among individuals and is equal to 1. Assume $D \mid G = E \sim Bernoulli(1/2)$.
    Let the probability of being treated conditional on being in the observational sample be $$\mathbb{P}(D = 1 \mid G = O) = \frac{1}{1 + \exp (-\log(K) \times \mathbb{I}(V > 0) - 0.5 \times \mathbb{I}(U > 0) )},$$ so we have $K-$latent-confoundedness. 
    Also, even after conditioning on $U$, individuals with larger $Y_i$ in observational sample are more likely to be treated, 
    which may again lead to over-estimation of $\tau$.
    
    \subsection{Simulation Results}
    
    In figure \ref{fig:fig1}, I show one realization of the lower and upper bounds when one adjust $K$ while fixing $H = 1.2$. This is a simple demonstration of the basic properties of these bounds. When $K = 1$, the latent-unconfoundedness assumption is valid, but since $H > 1$, sample comparability is violated, and therefore the lower bound and upper bound are different. As $K$ gets bigger, the identified set grows. In this realization, when $K = 1.2$, the lower bound is slightly larger than the true value of $\tau$, but if we take into account the uncertainty of estimation, we can construct a lower-CI for the lower bound which will contain the true value with high probability.    
    
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.6\textwidth]{../code/fig1.png}
        \caption{Demonstration of the lower and upper bounds for $\tau$: $H = 1.2, K$ varied}
        \label{fig:fig1}
    \end{figure}
    
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width = 0.6 \textwidth]{../code/fig2.png}
        \caption{Histogram of Simulated Bounds: $B = 2000, n = 2000, K = H = 1.2$}
        \label{fig:fig2}
    \end{figure}


	\begin{table}
        \centering
    \begin{threeparttable}
    \caption{Monte Carlo Results: $B = 2000, \tau = 1, K = H = 1$}
        \begin{tabular}{| c | c | c | c | c | c |}
        \hline $\mathrm{n}$ & $\widehat{\tau}^{-}(\widehat{\tau}^{+})$ & $\widehat{\sigma}^{-}(\widehat{\sigma}^{+})$ & \text {Std. dev. of } $\widehat{\tau}^{-}(\widehat{\tau}^{+})$ & \text {Coverage } \\
        \hline 500 & 1.038 & 0.727 & 0.732  & 0.955 \\
        1000 & 1.039 & 0.511 & 0.508  & 0.945 \\
        2000 & 1.029 & 0.362 & 0.352 & 0.947 \\
        \hline
        \end{tabular}
        \label{tab:tab1}
    \begin{tablenotes}
      \small
      \item Coverage statistics from $B = 2000$ simulations generated from the model specified above. The true ATE in the simulation is $\tau = 1$, and unobserved confounding is chosen such that simulation satisfies $K = H = 1$. Std. dev. of $\tau^-$ refers to standard deviation between simulation runs (likewise for $\tau^+$. Coverage is for estimated 95\% confidence intervals $(\hat{\tau}^- - 1.96 \hat{\sigma}^-, \hat{\tau}^+ + 1.96 \hat{\sigma}^+)$. In this case, $\tau^-$ and $\tau^+$ coincide. 
    \end{tablenotes}
  \end{threeparttable}
  \end{table}

  The finite sample coverage property of confidence intervals for different $n$ are demonstrated below in table \ref{tab:tab1} and table \ref{tab:tab2}. To examine the coverage properties of the proposed CIs, I consider two settings and simulate 2000 datasets under each: in the first setting, there is no confounding at all, so $K$ and $H$ are both set to 1.
    In the second one, we have both confounding effects, and $K = H = 1.2$. 
    Ideally, in the first setting, the CI should provide a coverage rate close to 95\%. The result for the first setting is presented in table \ref{tab:tab1}. When can see that the estimate is a little bit biased upward, but as $n$ gets larger the bias is reduced. The standard error of $\hat{\tau}^-(\hat{\tau}^+)$ is accurately estimated by $\hat{\sigma}^-(\hat{\sigma}^+)$.
    Because the true parameter $\tau$ is known from the simulation design, coverage statistics are estimable, and are presented for a number of choices of $n$ varying from 500 to 2000 in table \ref{tab:tab1}. The coverage rate is quite close to 95\% across all $n$, suggesting that the inference procedure is valid when the assumptions are all true.
    In the second setting, the naive estimate based on ACI(2020) will give a upward-biased estimate of $\tau$. In this case, our proposed method gives a valid 95\% confidence interval. 
    From table \ref{tab:tab2}, we can see that estimates of bounds and their standard errors are accurate, and the coverage rate is good even in small sample. 
    Meanwhile, the proposed CI is too conservative, since for small values of $K$ and $H$ it already provides coverage rate close to 100\%. There are two reasons for this phenomena. First, in the current setting the treatment effect is constant. Therefore, the $Y(1)$ is constrained by $Y(0)$ and cannot attain the worst case bounds. Second, even $\mathbb{E}[Y(1) \mid G = O]$ can attain its lower bound and $\mathbb{E}[Y(0) \mid G = O]$ can its upper bound, so that we have a tight lower bound for $\tau$, the upper bound of $\tau$ is surely not attained. The CI for the upper bound of $\tau$ contributes to the conservativeness by concentrating around a point larger than the true $\tau$. In figure \ref{fig:fig2}, I show the histogram of upper and lower bounds of $\tau$. The lower bounds concentrate slightly left to the true value, while the upper bounds concentrate around 2, which illustrates the two issues that I mentioned.
    

    \begin{table}
        \centering
    \begin{threeparttable}
    \caption{Monte Carlo Results: $B = 2000, \tau = 1, K = H = 1.2$}
        \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline $\mathrm{n}$ & $\widehat{\tau}^{-}$ & $\widehat{\sigma}^{-}$ & \text {Std. dev. of } $\widehat{\tau}^{-}$ & $\widehat{\tau}^{+}$ & $\widehat{\sigma}^{+}$ & \text {Std. dev. of } $\widehat{\tau}^{+}$ & \text {Coverage } \\
        \hline 500 & 0.704 & 0.745 & 0.746 & 1.985 & 0.744 & 0.748 & 0.989 \\
        1000 & 0.730 & 0.526 & 0.518 & 1.993 & 0.525 & 0.516 & 0.992 \\
        2000 & 0.749 & 0.370  & 0.376 & 2.000 & 0.371 & 0.375 & 0.996 \\
        \hline
        \end{tabular}
        \label{tab:tab2}
    \begin{tablenotes}
      \small
      \item Coverage statistics from $B = 2000$ simulations generated from the model specified above. The true ATE in the simulation is $\tau = 1$, and unobserved confounding is chosen such that simulation satisfies $K = H = 1.2$. Std. dev. of $\tau^-$ refers to standard deviation between simulation runs (likewise for $\tau^+$). Coverage is for estimated 95\% confidence intervals $(\hat{\tau}^- - 1.96 \hat{\sigma}^-, \hat{\tau}^+ + 1.96 \hat{\sigma}^+)$.
    \end{tablenotes}
  \end{threeparttable}
  \end{table}

  \section{CONCLUSION}

  In this paper, I study sensitivity analysis for estimation of long-term treatment effects.
  The method proposed in ACI(2020) relies on the assumptions of latent unconfoundedness and sample comparability, which is vulnerable to confounding effect when there are no further available data (covariates or instruments).
  I show how one can nonparametrically weaken multiple assumptions and derive identified sets in this case. For chosen sensitivity parameter, I derived identified sets (a interval characterizing the lower bound and upper bound for the ATE) under for the average long-term treatment effect in the observational sample, which is often the quantity of interest in policy making.
  They have simple and analytical characterization, and can be derived using change-of-measure technique together with convex duality. 
  Moreover, since the (partial) identification is constructive, estimators are also directly available from the sample analogue.
  Finally, I provide an example involving simulated data to illustrate the accuracy of estimation and inference procedure and its practical significance. The confidence interval provides valid coverage, but are too conservative as shown in the simulation study.

  There are still several remaining questions. 
  First, although the identification result can be directly extended to the cases where there are covariates, the current estimation procedure only works for the case where the cardinality of covariates is finite. Estimation in the presence of continuous covariates is a further research question. Second, unlike the method proposed in \textcite{masten2018identification}, 
  the current procedure is useful for deriving identified sets for ATE, but is not as useful when one is interested in quantile treatment effects. Investigating whether one can use the same idea to derive bounds on quantile treatment effects is also a helpful addition to current work. Finally, in this paper, the issue of correlated confounders is only briefly discussed and not addressed. In real-world applications, confounders for latent unconfoundedness and sample comparability could often be correlated and the researcher may have \textit{a priori} knowledge about the correlation between confounders. In this case, it's desirable to have a procedure of assessing sensitivity that take into account this piece of additional information. 
    
	\medskip
	
	\pagebreak
	\nocite{*}
	\printbibliography
	
	\pagebreak
	\appendix
	\section{Proofs}

	
	\duality*
	
	\begin{proof}
		The proof is similar to the one presented in \textcite{yadlowsky2018bounds}. 
		
		First, note that $\mathbb{E}[L(W(1))f(W(1)) \mid D=1]$ is a linear functional of $L(y)$ and thus, a convex functional. Next, the constraint on $L(w)$ are such that the constraint set is convex, so strong duality (see, for example, \textcite{luenberger1997optimization}) holds: by the principle of lagrangian, the original problem can be written as
		$$
		\begin{array}{ll}
		\sup_{\mu} & \inf _{L(w)} \quad \mathbb{E}[L(W(1))f(W(1)) \mid D=1] + \mu(1 -  \mathbb{E}L[W(1)\mid D = 1]) \\
		\text { s.t. }  & L(w) \geq 0, L(w) \leq K L(\tilde{w}) \text { for almost every } w, \tilde{w} \in \mathbb{R}
		\end{array}
		$$
		so the equivalence between the first two problem is established.
		
		The rest of the proof roughly follows the same argument as in \textcite{yadlowsky2018bounds}.
		
		Consider the problem of 
		$$
		\begin{array}{ll}
		\inf _{L(w)} & \quad \mathbb{E}[(f(W(1))-\mu) L(Y(1)) \mid D=1]+\mu \\
		\text { s.t. } &\quad L(w) \geq 0, L(w) \leq K L(\tilde{w}) \text { for all } y, \tilde{w} \in \mathbb{R}.
		\end{array}
		$$
		
		For fixed $\mu$, to make the quantity as small as possible, it must be that $L(W(1)) \propto K \Indc(W(1) - \mu \leq 0) + \Indc(W(1) - \mu > 0)$.
		
		Let $L^*(y) = c(K \Indc(f(W(1)) - \mu \leq 0) + \Indc(f(W(1)) - \mu > 0))$, with $c \geq 0$. The problem becomes
		$$
		\begin{array}{ll}
		\sup_{\mu} & \inf _{c \geq 0} \quad c\mathbb{E}\left[(f(W(1))-\mu)_{+}-K(f(W(1))-\mu)_{-} \mid D=1\right] + \mu \\
		\end{array}
		$$
		Suppose $\mathbb{E}\left[(f(W(1))-\mu)_{+}-K(f(W(1))-\mu)_{-} \mid D=1\right] < 0$, the term would be $-\infty$, so the above problem is equal to 
		\begin{equation*}
		\begin{array}{ll}
		\theta_1 \coloneqq & \sup _{\mu}  \mu \\
		& \text { s.t. }  \mathbb{E}\left[(f(W(1))-\mu)_{+}-K(f(W(1))-\mu)_{-} \mid D=1\right] \geq 0.
		\end{array}
		\end{equation*}
		It' s also direct to see that the supremum is attained when the equality holds.
	\end{proof}

	\lb*
	\begin{proof}

		\textbf{(a)} In this case, applying lemma \ref{duality} conditional on $S(1)$ and $G = O$, while taking $f$ as the identity function, for each $s$ we have a lower bound $\theta_1(s)$ for $\Ep[Y(1) \mid S(1) = s, D = 0, G = O]$ given by equation \ref{eq:eq1}. Integration over $S(1) | D = 0, G = O$ gives
		$$\mu^-_1 = \int \theta_1(s) \mathrm{d} P_{S(1) | D = 0, G = O}(s)$$
		which is a valid lower bound on $E[Y(1) \mid D = 0, G = O]$. 
		
		Since sample comparability is maintained, $P_{S(1) \mid G = O}(s) = P_{S(1) \mid G = E}(s) = P_{S(1) \mid D = 1, G = E}(s)$. The total probability formula, on the other hand, gives $$
		P_{S(1) \mid G = O}(s) = P_{S(1) \mid D= 0, G = O}(s) P(D = 0 \mid G = O) +  P_{S(1) \mid D=1, G = O}(s) P(D = 1 \mid G = O),
		$$
		so we can solve for $P_{S(1) \mid D = 0, G = O}(s)$. That gives equation \ref{eq:eq2}.
		
		\textbf{(b)}
		Now consider the more general case where sample comparability assumption might fail. We can no longer point identify $P_{S(1) \mid D = 0, G = O}$ in this case, since $P_{S(1) \mid G = O}$ not necessarily equals $P_{S(1) \mid G = E}$. 
		
		However, note that the quantity $\int \theta_1(s) \mathrm{d} P_{S(1) | D = 0, G = O}(s) = \Ep[\theta_1(S(1)) \mid D = 0, G = O]$ is just another conditional expectation with $\Ep[\theta_1(S(1)) \mid D = 0, G = E]$ point identified. Therefore, viewing $G$ as an assignment in lemma \ref{duality}, we can perform the change of measure again. Here the $f$ in lemma \ref{duality} is $\theta_1$.
		
		Consider the minimization of $\int \theta_1(s) \mathrm{d} P_{S(1) | D = 0, G = O}(s)$, subject to the constraint of $H-$comparability. Note that 
		\begin{align*}
		     \int \theta_1(s) \mathrm{d} P_{S(1) | D = 0, G = O}(s)
		     & = \int \theta_1(s) d \left( \frac{P_{S(1) \mid G = O}(s) - P_{S(1) \mid D = 1, G = O}(s) P(D = 1 \mid G = O)}{P(D = 0 \mid G = O)} \right),
		\end{align*}
		and only non-identified part is $\int \theta_1(s) \mathrm{d} P_{S(1) \mid G = O}$. But then an application of lemma \ref{duality} gives a lower bound for this quantity, as shown in \ref{eq:eq4}. Equation \ref{eq:eq3} then follows.
	\end{proof}
	

    \subsection{Detailed Proof of Consistency and Asymptotic Normality}
	\begin{restatable}{thm}{consistency}
		\label{consistency}
		$\hat{\theta}_{1,s} \overset{p}{\to} \theta_{1,s}$ for all $s \in \{s_1, \ldots, s_k\}$, and $\hat{\zeta}_1 \overset{p}{\to} \zeta_1$. 
	\end{restatable}
	
	\begin{proof}
		For each single $s_k$, the consistency follows from the fact that the following map is strictly increasing and continuous in $x$:
		$$\psi_x(y) = (y - x)_+ - K (y - x)_-.$$
		The $Z$-estimator is given by $\Ep_n[\psi_{\hat{\theta}(s)}(Y(1)) \mid S(1) = s, D = 1, G = O] = 0$, which, by Lemma 5.10 in \textcite{van2000asymptotic}, gives an consistent estimator for $\theta_1(s)$.
		
		For $\zeta_1$, define $$\phi_x(y) = (y - x)_+ - H (y - x)_-.$$
		\begin{equation*}
		\mathbb{E}_n\left[(\hat{\theta}_1(S(1))-\hat{\zeta}_1)_{+}-H(\hat{\theta}_1(S(1))-\hat{\zeta}_1)_{-} \mid D=1, G = E\right] = 0
		\end{equation*} 
		can be written as 
		\begin{equation*}
		\mathbb{E}_n\left[ \phi_{\hat{\zeta}_1}(\hat{\theta}_1(S(1)))\mid D=1, G = E\right] = 0.
		\end{equation*}
		Note that $\phi_{x}(y)$ is uniformly continous in each argument when we fix the other one. By continuous mapping theorem,
		\begin{equation*}
		\mathbb{E}_n\left[ \phi_{\hat{\zeta}_1}(\theta_1(S(1)))\mid D=1, G = E\right] = \littleop(1).
		\end{equation*}
		Then again by Lemma 5.10 in \textcite{van2000asymptotic}, we have the consistency of $\hat{\zeta}_1$.
	\end{proof}
	
	\begin{restatable}{thm}{asymnorm}
		\label{asymnorm}
		Suppose $P(Y(1) = \theta_{1,s_j} \mid S(1) = s_j, D = 1) = 0$ and $\zeta_1 \neq \theta_{1,s_j}$ for all $k = 1, \cdots, j$. Let $\pi = P(G = E)$. We have
		$$\sqrt{n}(\hat{\theta}_{1,s} - \theta_{1,s}) \overset{d}{\to} N(0,V_s)$$ for all $s \in \{s_1, \ldots, s_k\}$, and $$\sqrt{n}(\hat{\zeta}_1-\zeta_1) \overset{d}{\to} N(0,V_{\zeta}).$$ 
		Here, 
		$$V_s = \frac{\Ep[\psi_{\theta_{1,s}}(Y(1))^2 \mid S(1) = s, D = 1]}{(1-\pi) P(S(1)=s, D=1) [K P(Y(1) \leq \theta \mid S(1) = s, D = 1) + P(Y(1) > \theta \mid S(1) = s, D = 1)]^2}$$ and
		$$V_{\zeta} = \frac{\sum_{j=1}^k P(S(1) = s_j \mid D = 1)\phi_{\zeta_1}(\theta_{1,j})^2}{\pi P(D = 1)[H P(S(1) \leq \zeta_1 \mid D = 1) + P(S(1)  > \zeta_1 \mid D = 1)]^2}.$$
	\end{restatable}
	
	\begin{proof}
		Let $\pi = \Pr(G = E).$ Without loss of generality, encode $G = O$ as $G = 0$ and $G = E$ as $G = 1$. From the above proof in theorem \ref{consistency}, we can see that for each $\theta_1(s)$, we can establish the corresponding asymptotic variance.
		$$\Ep[\psi_{\theta_1(s)}(Y(1)) \mid S(1) = s, D = 1, G = O] = 0$$ is equivalent with 
		$$\Ep[\Indc(S_i(1) = s) D_i(1-G_i)\psi_{\theta_1(s)}(Y_i(1))] = 0.$$
		Let $Z_i = (Y(1), S(1), T, D, G)$. Define $m_{\theta_1(s)}(Z_i) = \Indc(S_i = s, D_i=1) G_i \psi_{\theta(s)}(Y(1)).$ The above moment condition is then $$\Ep[m_{\theta_1(s)}(Z_i)] = 0.$$ 
		Moreover, the moment for $S(1)$ is given by 
		$$\mathbb{E}\left[ D_i G_i \phi_{\zeta_1}(\theta_1(S_i(1)))\right] = 0.$$
		Rewrite the system of moments as follows:
		\begin{align*}
		\Ep[(1-G)\Indc(S(1) = s_1) D \psi_{\theta_{1,1}}(Y(1))] & = 0\\
		& \cdots \\
		\Ep[(1-G)\Indc(S(1) = s_k) D \psi_{\theta_{1,k}}(Y(1))] & = 0\\
		\Ep[(G \sum_{j =1}^k \Indc(S(1) = s_j) D \phi_{\zeta_1}(\theta_{1,j})] & = 0
		\end{align*}
		This is a just identified system with non-correlated moments.
		Let's first focus on the first $k$ equations.
		
		To prove asymptotic normality, I resort to Lemma 5.21 in \textcite{van2000asymptotic}. We need to verify all the conditions needed. The first one is Lipschitz continuity. This follows naturally since $\psi_{\theta}(y)$ is $K-$Lipschitz continuous. The constant function is certainly integrable, so we are done. The second one is differentiability. For the first $k$ equations, $P\psi_{\theta}$ can be shown to be differentiable. Too see this, consider the following simple demonstration: 
		\begin{align*}
		\frac{\partial}{\partial \theta}\Ep[\psi_{\theta}(Y(1))] & = \frac{\partial}{\partial \theta} \Ep[(Y(1) - \theta)_+ - K (Y(1) - \theta)_-].
		\end{align*}
		We have
		$$\frac{\partial}{\partial \theta} \Ep[(Y(1) - \theta)_+] = F_{Y(1)}(\theta) - 1$$
		and 
		$$\frac{\partial}{\partial \theta} \Ep[K(Y(1) - \theta)_-] = K F_{Y(1)}(\theta) $$
		by the absolute continuity of $F_{Y(1)}$.
		
		Applying Lemma 5.21 in \textcite{van2000asymptotic}, we have $\sqrt{n} (\hat{\theta}_1(s) - \theta_1(s)) \overset{d}{\to} N(0, V_s)$ where 
		$$V_s = \frac{\Ep[\psi_{\theta_{1,s}}(Y(1))^2 \mid S(1) = s, D = 1]}{(1-\pi) P(S(1)=s, D=1) [K P(Y(1) \leq \theta \mid S(1) = s, D = 1) + P(Y(1) > \theta \mid S(1) = s, D = 1)]^2}$$
		
		Now, for the asymptotic distribution of $\zeta$, the process is essentially similar and $$V_* = \frac{\sum_{j=1}^k P(S(1) = s_j)\phi_{\zeta_1}(\theta_{1,j})^2}{\pi P(D = 1)[H P(S(1) \leq \zeta_1 \mid T = 1) + P(S(1)  > \zeta_1 \mid D = 1)]^2}.$$
	\end{proof}
	
\end{document}